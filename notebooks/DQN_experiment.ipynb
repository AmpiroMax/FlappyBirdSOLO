{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.environ[\"DATA_PATH\"] = \"../assets/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.4.0 (SDL 2.26.4, Python 3.10.7)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from src.game.wrapped_flappy_bird import GameState\n",
    "from src.models.DoubleDQN import QualityEstimator, policy, ConvQualityEstimator\n",
    "from src.data.replay_memory import ReplayMemory\n",
    "from src.pipelines.utils import get_state, get_image_state\n",
    "from src.pipelines.train import optimize_model_double_dqn\n",
    "from src.data.shemas import ConfigData\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = {\n",
    "    \"sessions_num\": 10000,\n",
    "    \"gamma\": 0.95,\n",
    "    \"lr\": 1e-6,\n",
    "    \"lr_decay\": 0.997,\n",
    "    \"state_dim\": 5,\n",
    "    \"action_dim\": 2,\n",
    "    \"hid_channel\": [32, 32, 64],\n",
    "    \"hid_dim\": [64, 512],\n",
    "    \"eps_init\": 1,\n",
    "    \"eps_last\": 1e-4,\n",
    "    \"eps_max_iters\": 1000,  \n",
    "    \"temperature\": 1,\n",
    "    \"batch_size\":  32,\n",
    "    \"grad_clip\": 100,\n",
    "    \"memory_size\": 10000,\n",
    "    \"model_load_path\": None, #\"../models_backup/\",\n",
    "    \"model_version\": \"_v10\",\n",
    "    \"model_save_path\": \"../models/\",\n",
    "    \"model_swap_time\": 10,\n",
    "    \"max_session_score\": 150,\n",
    "    \"player_flap_acc\": -9,\n",
    "    \"dropout\": 0.2,\n",
    "    \"img_num_in_state\": 4,\n",
    "    \"device\": DEVICE\n",
    "}\n",
    "\n",
    "cfg = ConfigData(\n",
    "    **config_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_terminal = GameState()\n",
    "curr_flap_acc = cfg.player_flap_acc\n",
    "action_terminal.playerFlapAcc = curr_flap_acc\n",
    "\n",
    "if cfg.hid_channel is not None:\n",
    "    model_qa = ConvQualityEstimator(\n",
    "        in_channels=cfg.img_num_in_state,\n",
    "        action_dim=cfg.action_dim,\n",
    "        hid_channel=cfg.hid_channel,\n",
    "        hid_dims=cfg.hid_dim,\n",
    "        dropout=cfg.dropout\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    model_qb = ConvQualityEstimator(\n",
    "        in_channels=cfg.img_num_in_state,\n",
    "        action_dim=cfg.action_dim,\n",
    "        hid_channel=cfg.hid_channel,\n",
    "        hid_dims=cfg.hid_dim,\n",
    "        dropout=cfg.dropout\n",
    "    ).to(DEVICE)\n",
    "else:\n",
    "    model_qa = QualityEstimator(\n",
    "        cfg.state_dim,\n",
    "        cfg.action_dim,\n",
    "        cfg.hid_dim,\n",
    "        cfg.dropout\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    model_qb = QualityEstimator(\n",
    "        cfg.state_dim,\n",
    "        cfg.action_dim,\n",
    "        cfg.hid_dim,\n",
    "        cfg.dropout\n",
    "    ).to(DEVICE)\n",
    "\n",
    "if cfg.model_load_path is not None:\n",
    "    model_qa = torch.load(cfg.model_load_path+\"model_qa\"+cfg.model_version).to(DEVICE)\n",
    "    model_qb = torch.load(cfg.model_load_path+\"model_qb\"+cfg.model_version).to(DEVICE)\n",
    "\n",
    "optimizer_qa = optim.AdamW(model_qa.parameters(), lr=cfg.lr)\n",
    "optimizer_qb = optim.AdamW(model_qb.parameters(), lr=cfg.lr)\n",
    "\n",
    "scheduler_qa = torch.optim.lr_scheduler.LambdaLR(optimizer_qa, lr_lambda=lambda epoch: cfg.lr_decay ** epoch)\n",
    "scheduler_qb = torch.optim.lr_scheduler.LambdaLR(optimizer_qb, lr_lambda=lambda epoch: cfg.lr_decay ** epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(\n",
    "    model,\n",
    "    target_model,\n",
    "    state,\n",
    "    memory,\n",
    "    optimizer,\n",
    "    sheduler,\n",
    "    eps_update_iter,\n",
    "    cfg: ConfigData\n",
    "):  \n",
    "    tensor_state = torch.cat(list(state)).unsqueeze(0).to(cfg.device)\n",
    "    with torch.no_grad():\n",
    "        action, _ = policy(\n",
    "            model,\n",
    "            tensor_state,\n",
    "            epsilon=max((cfg.eps_init - cfg.eps_init / cfg.eps_max_iter * eps_update_iter), cfg.eps_last)\n",
    "        )            \n",
    "    _, reward, has_failed = action_terminal.frame_step(action)\n",
    "    reward = torch.tensor(reward, device=DEVICE).view(1, 1)\n",
    "    \n",
    "    if cfg.hid_channel is not None:\n",
    "        state.append(get_image_state(action_terminal))\n",
    "        next_tensor_state = torch.cat(list(state)).unsqueeze(0)\n",
    "        memory.push(tensor_state.to(\"cpu\"), action, next_tensor_state, reward)\n",
    "    else:\n",
    "        next_state = get_state(action_terminal)\n",
    "        memory.push(state, action, next_state, reward)\n",
    "        state = next_state\n",
    "    \n",
    "    loss_value = optimize_model_double_dqn(model, target_model, memory, optimizer, cfg)\n",
    "    sheduler.step()\n",
    "\n",
    "    return reward, has_failed, loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d19aafeee0841a38f406434538678ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Max\\Proga\\python_venv\\.venv\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 44\u001b[0m\n\u001b[0;32m     32\u001b[0m     reward, has_failed, loss_value \u001b[39m=\u001b[39m train_step(\n\u001b[0;32m     33\u001b[0m         model_qa,\n\u001b[0;32m     34\u001b[0m         model_qb,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     40\u001b[0m         cfg\n\u001b[0;32m     41\u001b[0m     )\n\u001b[0;32m     43\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 44\u001b[0m     reward, has_failed, loss_value \u001b[39m=\u001b[39m train_step(\n\u001b[0;32m     45\u001b[0m         model_qb,\n\u001b[0;32m     46\u001b[0m         model_qa,\n\u001b[0;32m     47\u001b[0m         state,\n\u001b[0;32m     48\u001b[0m         memory,\n\u001b[0;32m     49\u001b[0m         optimizer_qb,\n\u001b[0;32m     50\u001b[0m         scheduler_qb,\n\u001b[0;32m     51\u001b[0m         eps_update_iter_b,\n\u001b[0;32m     52\u001b[0m         cfg\n\u001b[0;32m     53\u001b[0m     )\n\u001b[0;32m     54\u001b[0m iteration \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     56\u001b[0m \u001b[39mif\u001b[39;00m loss_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[6], line 30\u001b[0m, in \u001b[0;36mtrain_step\u001b[1;34m(model, target_model, state, memory, optimizer, sheduler, eps_update_iter, cfg)\u001b[0m\n\u001b[0;32m     27\u001b[0m     memory\u001b[39m.\u001b[39mpush(state, action, next_state, reward)\n\u001b[0;32m     28\u001b[0m     state \u001b[39m=\u001b[39m next_state\n\u001b[1;32m---> 30\u001b[0m loss_value \u001b[39m=\u001b[39m optimize_model_double_dqn(model, target_model, memory, optimizer, cfg)\n\u001b[0;32m     31\u001b[0m sheduler\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     33\u001b[0m \u001b[39mreturn\u001b[39;00m reward, has_failed, loss_value\n",
      "File \u001b[1;32mc:\\max\\proga\\grishin_ml\\flappybirdsolo\\src\\pipelines\\train.py:82\u001b[0m, in \u001b[0;36moptimize_model_double_dqn\u001b[1;34m(model, target_model, memory, optimizer, cfg)\u001b[0m\n\u001b[0;32m     80\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     81\u001b[0m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_value_(model\u001b[39m.\u001b[39mparameters(), cfg\u001b[39m.\u001b[39mgrad_clip)\n\u001b[1;32m---> 82\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m     84\u001b[0m \u001b[39mreturn\u001b[39;00m loss\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Max\\Proga\\python_venv\\.venv\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:65\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m instance\u001b[39m.\u001b[39m_step_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     64\u001b[0m wrapped \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance, \u001b[39mcls\u001b[39m)\n\u001b[1;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Max\\Proga\\python_venv\\.venv\\lib\\site-packages\\torch\\optim\\optimizer.py:113\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m    112\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 113\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Max\\Proga\\python_venv\\.venv\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Max\\Proga\\python_venv\\.venv\\lib\\site-packages\\torch\\optim\\adamw.py:161\u001b[0m, in \u001b[0;36mAdamW.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    157\u001b[0m             max_exp_avg_sqs\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mmax_exp_avg_sq\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m    159\u001b[0m         state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m--> 161\u001b[0m     adamw(params_with_grad,\n\u001b[0;32m    162\u001b[0m           grads,\n\u001b[0;32m    163\u001b[0m           exp_avgs,\n\u001b[0;32m    164\u001b[0m           exp_avg_sqs,\n\u001b[0;32m    165\u001b[0m           max_exp_avg_sqs,\n\u001b[0;32m    166\u001b[0m           state_steps,\n\u001b[0;32m    167\u001b[0m           amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    168\u001b[0m           beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    169\u001b[0m           beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    170\u001b[0m           lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    171\u001b[0m           weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    172\u001b[0m           eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    173\u001b[0m           maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    174\u001b[0m           foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    175\u001b[0m           capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m    177\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Max\\Proga\\python_venv\\.venv\\lib\\site-packages\\torch\\optim\\adamw.py:218\u001b[0m, in \u001b[0;36madamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    216\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adamw\n\u001b[1;32m--> 218\u001b[0m func(params,\n\u001b[0;32m    219\u001b[0m      grads,\n\u001b[0;32m    220\u001b[0m      exp_avgs,\n\u001b[0;32m    221\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    222\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    223\u001b[0m      state_steps,\n\u001b[0;32m    224\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    225\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    226\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    227\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    228\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    229\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    230\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[0;32m    231\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable)\n",
      "File \u001b[1;32mc:\\Max\\Proga\\python_venv\\.venv\\lib\\site-packages\\torch\\optim\\adamw.py:267\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m    266\u001b[0m exp_avg\u001b[39m.\u001b[39mmul_(beta1)\u001b[39m.\u001b[39madd_(grad, alpha\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta1)\n\u001b[1;32m--> 267\u001b[0m exp_avg_sq\u001b[39m.\u001b[39;49mmul_(beta2)\u001b[39m.\u001b[39maddcmul_(grad, grad, value\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta2)\n\u001b[0;32m    269\u001b[0m \u001b[39mif\u001b[39;00m capturable:\n\u001b[0;32m    270\u001b[0m     step \u001b[39m=\u001b[39m step_t\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_rewards = []\n",
    "\n",
    "eps_update_iter_a = 0\n",
    "eps_update_iter_b = 0\n",
    "memory = ReplayMemory(cfg.memory_size)\n",
    "\n",
    "iteration = 0\n",
    "\n",
    "for session_idx in tqdm(range(cfg.sessions_num)):\n",
    "    \n",
    "    # Initialize new session\n",
    "    has_failed = False\n",
    "    max_score = 0\n",
    "    input_action = 0\n",
    "    total_reward = 0.0\n",
    "    mean_loss = 0.0\n",
    "    session_len = 0\n",
    "    action_terminal.playerFlapAcc = curr_flap_acc\n",
    "    \n",
    "    action_terminal.frame_step(0)\n",
    "    \n",
    "    if cfg.hid_channel is not None:\n",
    "        state = deque([], maxlen=cfg.img_num_in_state)\n",
    "        for _ in range(cfg.img_num_in_state):\n",
    "            state.append(get_image_state(action_terminal))\n",
    "    else:\n",
    "        state = get_state(action_terminal)\n",
    "    \n",
    "    # Session run\n",
    "    while not has_failed:\n",
    "        if iteration % (2 * cfg.model_swap_time) < cfg.model_swap_time:\n",
    "            reward, has_failed, loss_value = train_step(\n",
    "                model_qa,\n",
    "                model_qb,\n",
    "                state,\n",
    "                memory,\n",
    "                optimizer_qa,\n",
    "                scheduler_qa,\n",
    "                eps_update_iter_a,\n",
    "                cfg\n",
    "            )\n",
    "            \n",
    "        else:\n",
    "            reward, has_failed, loss_value = train_step(\n",
    "                model_qb,\n",
    "                model_qa,\n",
    "                state,\n",
    "                memory,\n",
    "                optimizer_qb,\n",
    "                scheduler_qb,\n",
    "                eps_update_iter_b,\n",
    "                cfg\n",
    "            )\n",
    "        iteration += 1\n",
    "        \n",
    "        if loss_value is not None:\n",
    "            mean_loss += loss_value\n",
    "            session_len += 1\n",
    "            \n",
    "\n",
    "        total_reward += reward\n",
    "        max_score = max(max_score, action_terminal.score)\n",
    "        if max_score >= cfg.max_session_score:\n",
    "            break\n",
    "    \n",
    "    # Post session updates\n",
    "    if (session_idx + 1) % 200 == 0:\n",
    "        torch.save(model_qa, cfg.model_save_path+\"model_qa\")\n",
    "        torch.save(model_qb, cfg.model_save_path+\"model_qb\")\n",
    "\n",
    "    if (session_idx + 1) % 2000 == 0 and curr_flap_acc > -9:\n",
    "        curr_flap_acc -= 1\n",
    "        action_terminal.playerFlapAcc = curr_flap_acc\n",
    "        eps_update_iter_a = 0\n",
    "        eps_update_iter_b = 0  \n",
    "            \n",
    "    eps_update_iter_a += 1\n",
    "    eps_update_iter_b += 1\n",
    "    \n",
    "    total_rewards += [total_reward]   \n",
    "    if session_len > 0: \n",
    "        mean_loss /= session_len\n",
    "    \n",
    "    writer.add_scalar(f\"Total reward {cfg=}\", total_reward, session_idx)\n",
    "    writer.add_scalar(f\"Max score {cfg=}\", max_score, session_idx)\n",
    "    writer.add_scalar(f\"Mean session loss {cfg=}\", mean_loss, session_idx)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
