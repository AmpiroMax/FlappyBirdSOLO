{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.environ[\"DATA_PATH\"] = \"../assets/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.4.0 (SDL 2.26.4, Python 3.10.7)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from src.game.wrapped_flappy_bird import GameState\n",
    "from src.models.DoubleDQN import QualityEstimator, policy\n",
    "from src.data.replay_memory import ReplayMemory\n",
    "from src.pipelines.utils import get_state\n",
    "from src.pipelines.train import optimize_model\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"sessions_num\": 10000,\n",
    "    \"gamma\": 0.99,\n",
    "    \"lr\": 1e-4,\n",
    "    \"lr_decay\": 0.998,\n",
    "    \"state_dim\": 4,\n",
    "    \"action_dim\": 2,\n",
    "    \"hid_dim\": [64, 128, 128, 64],\n",
    "    \"eps_init\": 0.05,\n",
    "    \"eps_last\": 1e-4,\n",
    "    \"eps_max_iters\": 250,  \n",
    "    \"temperature\": 1,\n",
    "    \"batch_size\": 300,\n",
    "    \"grad_clip\": 50,\n",
    "    \"memory_size\": 100000,\n",
    "    \"model_load_path\": \"../models_backup/\",\n",
    "    \"model_version\": \"_v04\",\n",
    "    \"model_save_path\": \"../models/\",\n",
    "    \"model_swap_time\": 10,\n",
    "    \"max_session_score\": 150\n",
    "}\n",
    "\n",
    "MODEL_SAVE_PATH = config[\"model_save_path\"]\n",
    "MEMORY_SIZE = config[\"memory_size\"]\n",
    "GRAD_CLIP = config[\"grad_clip\"]\n",
    "BATCH_SIZE = config[\"batch_size\"]\n",
    "LR = config[\"lr\"]\n",
    "LR_DECAY = config[\"lr_decay\"]\n",
    "GAMMA = config[\"gamma\"]\n",
    "EPS_INIT = config[\"eps_init\"]\n",
    "EPS_LAST = config[\"eps_last\"]\n",
    "EPS_MAX_ITERS = config[\"eps_max_iters\"]\n",
    "TEMP = config[\"temperature\"]\n",
    "SESSIONS_NUM = config[\"sessions_num\"]\n",
    "MODEL_SWAP_TIME = config[\"model_swap_time\"]\n",
    "MAX_SESSION_SCORE = config[\"max_session_score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_terminal = GameState()\n",
    "action_terminal.playerFlapAcc = -7\n",
    "\n",
    "model_qa = QualityEstimator(\n",
    "    config[\"state_dim\"],\n",
    "    config[\"action_dim\"],\n",
    "    config[\"hid_dim\"]\n",
    ").to(DEVICE)\n",
    "\n",
    "model_qb = QualityEstimator(\n",
    "    config[\"state_dim\"],\n",
    "    config[\"action_dim\"],\n",
    "    config[\"hid_dim\"]\n",
    ").to(DEVICE)\n",
    "\n",
    "if config[\"model_load_path\"] is not None:\n",
    "    model_qa = torch.load(config[\"model_load_path\"]+\"model_qa\"+config[\"model_version\"])\n",
    "    model_qb = torch.load(config[\"model_load_path\"]+\"model_qb\"+config[\"model_version\"])\n",
    "\n",
    "optimizer_qa = optim.Adam(model_qa.parameters(), lr=LR)\n",
    "optimizer_qb = optim.Adam(model_qb.parameters(), lr=LR)\n",
    "\n",
    "scheduler_qa = torch.optim.lr_scheduler.LambdaLR(optimizer_qa, lr_lambda=lambda epoch: LR_DECAY ** epoch)\n",
    "scheduler_qb = torch.optim.lr_scheduler.LambdaLR(optimizer_qb, lr_lambda=lambda epoch: LR_DECAY ** epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d93d6602142046f7b98ec828587e87b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Max\\Proga\\python_venv\\.venv\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 48\u001b[0m\n\u001b[0;32m     45\u001b[0m memory\u001b[39m.\u001b[39mpush(state, action, next_state, reward)\n\u001b[0;32m     46\u001b[0m state \u001b[39m=\u001b[39m next_state\n\u001b[1;32m---> 48\u001b[0m optimize_model(model_qb, model_qa, memory, optimizer_qb, BATCH_SIZE, GAMMA, GRAD_CLIP)\n\u001b[0;32m     49\u001b[0m scheduler_qb\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     50\u001b[0m eps_update_iter_b \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\max\\proga\\grishin_ml\\flappybirdsolo\\src\\pipelines\\train.py:48\u001b[0m, in \u001b[0;36moptimize_model\u001b[1;34m(model, target_model, memory, optimizer, batch_size, gamma, grad_clip)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[39m# Optimize the model\u001b[39;00m\n\u001b[0;32m     47\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> 48\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     49\u001b[0m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_value_(model\u001b[39m.\u001b[39mparameters(), grad_clip)\n\u001b[0;32m     50\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Max\\Proga\\python_venv\\.venv\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Max\\Proga\\python_venv\\.venv\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_rewards = []\n",
    "memory = ReplayMemory(MEMORY_SIZE)\n",
    "\n",
    "eps_update_iter_a = 0\n",
    "eps_update_iter_b = 0\n",
    "\n",
    "for session_idx in tqdm(range(SESSIONS_NUM)):\n",
    "    \n",
    "    # Initialize new session\n",
    "    is_failed = False\n",
    "    max_score = 0\n",
    "    input_action = 0\n",
    "    total_reward = 0.0\n",
    "    state = get_state(action_terminal)\n",
    "    \n",
    "    # Session run\n",
    "    while not is_failed:\n",
    "        if session_idx % (2 * MODEL_SWAP_TIME) < MODEL_SWAP_TIME:\n",
    "            action, predicted_reward = policy(\n",
    "                model_qa,\n",
    "                state,\n",
    "                epsilon=max((EPS_INIT - EPS_INIT / EPS_MAX_ITERS * eps_update_iter_a), EPS_LAST)\n",
    "            )            \n",
    "            _, reward, is_failed = action_terminal.frame_step(action)\n",
    "            reward = torch.tensor([reward], device=DEVICE)\n",
    "            next_state = get_state(action_terminal)\n",
    "            \n",
    "            memory.push(state, action, next_state, reward)\n",
    "            state = next_state\n",
    "\n",
    "            optimize_model(model_qa, model_qb, memory, optimizer_qa, BATCH_SIZE, GAMMA, GRAD_CLIP)\n",
    "            scheduler_qa.step()\n",
    "            eps_update_iter_a += 1\n",
    "        else:\n",
    "            action, predicted_reward = policy(\n",
    "                model_qb,\n",
    "                state,\n",
    "                epsilon=max((EPS_INIT - EPS_INIT / EPS_MAX_ITERS * eps_update_iter_b), EPS_LAST)\n",
    "            )\n",
    "            _, reward, is_failed = action_terminal.frame_step(action)\n",
    "            reward = torch.tensor([reward], device=DEVICE)\n",
    "            next_state = get_state(action_terminal)\n",
    "            \n",
    "            memory.push(state, action, next_state, reward)\n",
    "            state = next_state\n",
    "\n",
    "            optimize_model(model_qb, model_qa, memory, optimizer_qb, BATCH_SIZE, GAMMA, GRAD_CLIP)\n",
    "            scheduler_qb.step()\n",
    "            eps_update_iter_b += 1\n",
    "\n",
    "        total_reward += reward\n",
    "        max_score = max(max_score, action_terminal.score)\n",
    "        if max_score >= MAX_SESSION_SCORE:\n",
    "            break\n",
    "    \n",
    "    # Post session updates\n",
    "    if (session_idx + 1) % 1000 == 0:\n",
    "        torch.save(model_qa, MODEL_SAVE_PATH+\"model_qa\")\n",
    "        torch.save(model_qb, MODEL_SAVE_PATH+\"model_qb\")\n",
    "\n",
    "    if (session_idx + 1) % 500 == 0 and action_terminal.playerFlapAcc > -9:\n",
    "        action_terminal.playerFlapAcc -= 1\n",
    "        eps_update_iter_a = 0\n",
    "        eps_update_iter_b = 0\n",
    "        print(action_terminal.playerFlapAcc)        \n",
    "        \n",
    "    total_rewards += [total_reward]    \n",
    "    writer.add_scalar(f\"Total reward {config=}\", total_reward, session_idx)\n",
    "    writer.add_scalar(f\"Max score {config=}\", max_score, session_idx)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
